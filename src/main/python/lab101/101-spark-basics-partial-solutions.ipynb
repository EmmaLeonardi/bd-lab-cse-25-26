{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7243a-b7d3-4e47-b3ec-49cafdebada5",
   "metadata": {},
   "source": [
    "# 101 Spark basics\n",
    "\n",
    "The goal of this lab is to get familiar with Spark programming.\n",
    "\n",
    "- Scala\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "    - [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "    - [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "- Python\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/3.5.0/rdd-programming-guide.html)\n",
    "    - [All RDD APIs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.RDD.html)\n",
    "\n",
    "Use `Tab` for autocompletion, `Shift+Tab` for documentation."
   ]
  },
  {
   "cell_type": "code",
   "id": "11bb4f39692d0432",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2e281923de95ede",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Local Spark\") \\\n",
    "    .config('spark.ui.port', '4040') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f9a0be28e5e823",
   "metadata": {},
   "source": [
    "## 101-1 Spark warm-up\n",
    "\n",
    "Load the ```capra``` and ```divinacommedia``` datasets and try the following actions:\n",
    "- Show their content (```collect```)\n",
    "- Count their rows (```count```)\n",
    "- Split phrases into words (```map``` or ```flatMap```; what’s the difference?)\n",
    "- Check the results (remember: evaluation is lazy)\n",
    "- Try the ```toDebugString``` function to check the execution plan\n",
    "    - In PySpark, use ```toDebugString().decode(\"unicode_escape\")```"
   ]
  },
  {
   "cell_type": "code",
   "id": "cbc4b5cee4a93755",
   "metadata": {},
   "source": [
    "rddCapra = sc.textFile(\"../../../../datasets/capra.txt\")\n",
    "rddDC = sc.textFile(\"../../../../datasets/divinacommedia.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82df92012331ab61",
   "metadata": {},
   "source": [
    "rddCapraWords1 = rddCapra.map(lambda x : x.split(\" \") )\n",
    "rddCapraWords1.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "605c9837173b267d",
   "metadata": {},
   "source": [
    "rddCapraWords1.count()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "277b58790bcfd931",
   "metadata": {},
   "source": [
    "rddCapraWords2 = rddCapra.flatMap(lambda x : x.split(\" \") )\n",
    "rddCapraWords2.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37cf925abda9b5c",
   "metadata": {},
   "source": [
    "rddCapraWords2.count()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "251a1761200725bf",
   "metadata": {},
   "source": [
    "rddL = rddCapra. \\\n",
    "   flatMap(lambda x : x.split(\" \") ). \\\n",
    "   map(lambda x : (x,1)). \\\n",
    "   reduceByKey(lambda x,y : x+y)\n",
    "print(rddL.toDebugString().decode(\"unicode_escape\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84d04659ace472d8",
   "metadata": {},
   "source": [
    "## 101-2 Basic Spark jobs\n",
    "\n",
    "Implement on Spark the following jobs and test them on both capra and divinacommedia datasets.\n",
    "\n",
    "- **Word count**: count the number of occurrences of each word\n",
    "  - Result: (sopra, 1), (la, 4), …\n",
    "- **Word length count**: count the number of occurrences of words of given lengths\n",
    "  - Result: (2, 4), (5, 8)\n",
    "- Count the average length of words given their first letter (i.e., words that begin with \"s\" have an average length of 5)\n",
    "  - Result: (s, 5), (l, 2), …\n",
    "- Return the inverted index of words (i.e., for each word, list the numbers of lines in which they appear)\n",
    "  - Result: (sopra, (0)), (la, (0, 1)), ...\n",
    "\n",
    "Also, check how sorting works and try to sort key-value RDDs by descending values."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ab6b26a927a7d91",
   "metadata": {},
   "source": [
    "# Word count\n",
    "rddCapra. \\\n",
    "  flatMap(lambda x : x.split(\" \") ). \\\n",
    "  map(lambda x : (x,1)). \\\n",
    "  reduceByKey(lambda x,y : x + y). \\\n",
    "  map(lambda kv: (kv[1],kv[0])). \\\n",
    "  sortByKey(False). \\\n",
    "  collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a163112f6f1996d",
   "metadata": {},
   "source": [
    "# Word length count\n",
    "rddCapra. \\\n",
    "  flatMap(lambda x : x.split(\" \") ). \\\n",
    "  map(lambda x : (len(x),1)). \\\n",
    "  reduceByKey(lambda x,y : x + y). \\\n",
    "  collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f920336ead88eb0",
   "metadata": {},
   "source": [
    "# Average word length by initial\n",
    "rddCapra. \\\n",
    "  flatMap(lambda x : x.split(\" \") ). \\\n",
    "  filter(lambda x : len(x)>0 ). \\\n",
    "  map(lambda x : (x[0:1].lower(), (1,len(x)))). \\\n",
    "  reduceByKey(lambda x, y : (x[0] + y[0], x[1] + y[1])). \\\n",
    "  mapValues(lambda v : v[1]/v[0]). \\\n",
    "  collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Average word length by initial (alternative on the final map)\n",
    "rddCapra. \\\n",
    "  flatMap(lambda x : x.split(\" \") ). \\\n",
    "  filter(lambda x : len(x)>0 ). \\\n",
    "  map(lambda x : (x[0:1].lower(), (1,len(x)))). \\\n",
    "  reduceByKey(lambda x, y : (x[0] + y[0], x[1] + y[1])). \\\n",
    "  map(lambda kv : (kv[0], kv[1][1]/kv[1][0])). \\\n",
    "  collect()"
   ],
   "id": "fce60ea554b154a"
  },
  {
   "cell_type": "code",
   "id": "c0e2a91002beea79",
   "metadata": {},
   "source": [
    "# Inverted index (word-based offset)\n",
    "rddCapra. \\\n",
    "  flatMap(lambda x : x.split(\" \") ). \\\n",
    "  zipWithIndex()\n",
    "# ... continue from here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f2686f5c972e3e4",
   "metadata": {},
   "source": [
    "# Inverted index (sentence-based offset)\n",
    "rddCapra. \\\n",
    "  zipWithIndex()\n",
    "# ... continue from here\n",
    "# Hint: associating sentence-based offsets to words can be done either via Python's list-comprehension or via the same Spark's transformations you have already used"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
