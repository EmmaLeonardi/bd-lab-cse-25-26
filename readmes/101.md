# 101 Spark basics with notebooks

The goal of this lab is to get familiar with Spark programming, implementing some basic map-reduce jobs and deploying them locally.

We will begin by working with Spark in an interactive way using notebooks.
- *Problem*: notebooks work in a Python environment. There are kernels enabling Scala programming (e.g., spylon-kernel), 
but 1) they are often problematic, 2) IDEs get easily confused.
- *Solution*: we will use PySpark (the Python version of Spark) when using notebooks. The APIs are pretty much identical.

## Configuration

### 1. Install

- Install Docker Desktop (already done in laboratory computers)
- Launch Docker Desktop
- Download the Docker image with Spark 3.5.0 ([reference](https://hub.docker.com/r/jupyter/all-spark-notebook))
    ```bash
    docker pull jupyter/all-spark-notebook:spark-3.5.0
    ```

### 2. Run
 
- Open the **PowerShell** in the project main directory (lab-cse-25-26)
  - Launch the Docker image
    ```bash
    docker run -d `
      -p 8888:8888 `
      -p 4040:4040 `
      -p 18080:18080 `
      -v "${PWD}:/home/jovyan/work" `
      jupyter/all-spark-notebook:spark-3.5.0
    ```
- Open Docker Desktop > Open the running container > Access the Jupyter notebook by clicking on the link that looks like
  `http://127.0.0.1:8888/lab?token=12345etc`

Note: The Jupyter notebook can be accessed via IntelliJ IDEA as well, but autocompletion doesn't work very well. 


### 3. Stop

You can easily stop and restart the container from Docker Desktop


## Exercise

All instructions are in the notebook environment [here](../src/main/python/lab101/101-spark-basics.ipynb).